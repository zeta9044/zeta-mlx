# MLX LLM Server 설정 파일 예시
# 사용하려면 config.yaml로 복사하세요: cp config.yaml.example config.yaml

server:
  host: 0.0.0.0
  port: 9044

models:
  default: qwen3-8b  # 기본 모델 별칭
  max_loaded: 2       # 동시 로드 최대 모델 수

  available:
    qwen3-8b:
      path: mlx-community/Qwen3-8B-4bit
      context_length: 32768
      quantization: 4bit
      description: "Qwen3 8B - 범용 대화"

    qwen3-4b:
      path: mlx-community/Qwen3-4B-4bit
      context_length: 32768
      quantization: 4bit
      description: "Qwen3 4B - 경량 빠른 응답"

    qwen2.5-7b:
      path: mlx-community/Qwen2.5-7B-Instruct-4bit
      context_length: 32768
      quantization: 4bit
      description: "Qwen2.5 7B - 긴 컨텍스트"

    llama3.2-3b:
      path: mlx-community/Llama-3.2-3B-Instruct-4bit
      context_length: 8192
      quantization: 4bit
      description: "Llama 3.2 3B - 경량"

inference:
  max_tokens: 2048
  temperature: 0.7
  top_p: 0.9
  stop_sequences: []
